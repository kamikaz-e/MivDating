#!/usr/bin/env python3
"""
RAG –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞ MivDating
–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç README.md –∏ —Ñ–∞–π–ª—ã –∏–∑ project/docs/ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–µ
"""

import os
import json
import requests
from pathlib import Path
from typing import List, Dict, Tuple

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
OLLAMA_URL = "http://localhost:11434"
EMBEDDING_MODEL = "nomic-embed-text"
CHUNK_SIZE = 512
CHUNK_OVERLAP = 128
DOCS_DIR = Path(__file__).parent
PROJECT_ROOT = DOCS_DIR.parent.parent
INDEX_FILE = DOCS_DIR / "rag_index.json"


class DocumentChunk:
    """–ß–∞–Ω–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""

    def __init__(self, content: str, source: str, chunk_index: int):
        self.content = content
        self.source = source
        self.chunk_index = chunk_index
        self.embedding = None

    def to_dict(self) -> dict:
        return {
            "content": self.content,
            "source": self.source,
            "chunk_index": self.chunk_index,
            "embedding": self.embedding
        }


def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º

    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        overlap: –†–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è

    Returns:
        –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
    """
    chunks = []
    start = 0
    text_length = len(text)

    while start < text_length:
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start += chunk_size - overlap

    return chunks


def get_embedding(text: str, model: str = EMBEDDING_MODEL) -> List[float]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ Ollama API

    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏

    Returns:
        –í–µ–∫—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
    """
    url = f"{OLLAMA_URL}/api/embeddings"
    payload = {
        "model": model,
        "prompt": text
    }

    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()
        return response.json()["embedding"]
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
        return []


def load_documents() -> List[Tuple[str, str]]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏

    Returns:
        –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–∏–º—è_—Ñ–∞–π–ª–∞, —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ)
    """
    documents = []

    # README.md –∏–∑ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
    readme_path = PROJECT_ROOT / "README.md"
    if readme_path.exists():
        with open(readme_path, 'r', encoding='utf-8') as f:
            content = f.read()
            documents.append(("README.md", content))
            print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω README.md ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤)")

    # –í—Å–µ .md —Ñ–∞–π–ª—ã –∏–∑ project/docs/
    for md_file in DOCS_DIR.glob("*.md"):
        with open(md_file, 'r', encoding='utf-8') as f:
            content = f.read()
            documents.append((md_file.name, content))
            print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω {md_file.name} ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤)")

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≥–∞–π–¥—ã –∏–∑ –∫–æ—Ä–Ω—è
    for guide_name in ["RAG_COMPLETE_GUIDE.md", "RAG_FILTERING_GUIDE.md",
                       "TESTING_GUIDE.md", "CHANGES_SUMMARY.md"]:
        guide_path = PROJECT_ROOT / guide_name
        if guide_path.exists():
            with open(guide_path, 'r', encoding='utf-8') as f:
                content = f.read()
                documents.append((guide_name, content))
                print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω {guide_name} ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤)")

    return documents


def index_documents() -> List[DocumentChunk]:
    """
    –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø—Ä–æ–µ–∫—Ç–∞

    Returns:
        –°–ø–∏—Å–æ–∫ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
    """
    print("=== –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞ ===\n")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
    documents = load_documents()
    print(f"\n–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}\n")

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –∏ —Å–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    all_chunks = []
    total_chunks = 0

    for doc_name, content in documents:
        print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {doc_name}...")

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        text_chunks = chunk_text(content)
        print(f"  –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(text_chunks)}")

        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
        for i, chunk_content in enumerate(text_chunks):
            chunk = DocumentChunk(chunk_content, doc_name, i)

            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            embedding = get_embedding(chunk_content)
            if embedding:
                chunk.embedding = embedding
                all_chunks.append(chunk)
                total_chunks += 1

                if (i + 1) % 5 == 0:
                    print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {i + 1}/{len(text_chunks)}")

        print(f"  ‚úì –ì–æ—Ç–æ–≤–æ\n")

    print(f"=== –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ ===")
    print(f"–í—Å–µ–≥–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {total_chunks}\n")

    return all_chunks


def save_index(chunks: List[DocumentChunk]):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω–¥–µ–∫—Å –≤ JSON —Ñ–∞–π–ª

    Args:
        chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    index_data = {
        "chunks": [chunk.to_dict() for chunk in chunks],
        "metadata": {
            "total_chunks": len(chunks),
            "chunk_size": CHUNK_SIZE,
            "chunk_overlap": CHUNK_OVERLAP,
            "embedding_model": EMBEDDING_MODEL
        }
    }

    with open(INDEX_FILE, 'w', encoding='utf-8') as f:
        json.dump(index_data, f, ensure_ascii=False, indent=2)

    print(f"‚úì –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {INDEX_FILE}")
    print(f"  –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {INDEX_FILE.stat().st_size / 1024:.2f} KB")


def cosine_similarity(v1: List[float], v2: List[float]) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏

    Args:
        v1, v2: –í–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è

    Returns:
        –ó–Ω–∞—á–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ –æ—Ç -1 –¥–æ 1
    """
    dot_product = sum(a * b for a, b in zip(v1, v2))
    magnitude1 = sum(a * a for a in v1) ** 0.5
    magnitude2 = sum(b * b for b in v2) ** 0.5

    if magnitude1 == 0 or magnitude2 == 0:
        return 0.0

    return dot_product / (magnitude1 * magnitude2)


def search(query: str, top_k: int = 5) -> List[Dict]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É

    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        –°–ø–∏—Å–æ–∫ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
    """
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å
    if not INDEX_FILE.exists():
        print("–û—à–∏–±–∫–∞: –∏–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é —Å–Ω–∞—á–∞–ª–∞.")
        return []

    with open(INDEX_FILE, 'r', encoding='utf-8') as f:
        index_data = json.load(f)

    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
    print(f"–ü–æ–∏—Å–∫: '{query}'")
    query_embedding = get_embedding(query)

    if not query_embedding:
        print("–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞")
        return []

    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –∫–∞–∂–¥—ã–º —á–∞–Ω–∫–æ–º
    results = []
    for chunk_data in index_data["chunks"]:
        similarity = cosine_similarity(query_embedding, chunk_data["embedding"])
        results.append({
            "content": chunk_data["content"],
            "source": chunk_data["source"],
            "chunk_index": chunk_data["chunk_index"],
            "score": similarity
        })

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é score
    results.sort(key=lambda x: x["score"], reverse=True)

    return results[:top_k]


def test_search():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É"""
    print("\n=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞ ===\n")

    test_queries = [
        "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç RAG —Å–∏—Å—Ç–µ–º–∞?",
        "–ì–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è IndexingService?",
        "–ö–∞–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É FilterConfig?",
        "–°—Ç–∏–ª—å –∫–æ–¥–∞ –¥–ª—è Composable —Ñ—É–Ω–∫—Ü–∏–π"
    ]

    for query in test_queries:
        print(f"\nüìù –ó–∞–ø—Ä–æ—Å: {query}")
        results = search(query, top_k=3)

        for i, result in enumerate(results, 1):
            print(f"\n  [{i}] Score: {result['score']:.3f} | –ò—Å—Ç–æ—á–Ω–∏–∫: {result['source']}")
            preview = result['content'][:150].replace('\n', ' ')
            print(f"      {preview}...")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import sys

    if len(sys.argv) > 1:
        command = sys.argv[1]

        if command == "index":
            # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            chunks = index_documents()
            save_index(chunks)

        elif command == "search":
            # –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É
            if len(sys.argv) < 3:
                print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python rag_indexer.py search '–≤–∞—à –∑–∞–ø—Ä–æ—Å'")
                return

            query = sys.argv[2]
            results = search(query, top_k=5)

            print(f"\n–ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}\n")
            for i, result in enumerate(results, 1):
                print(f"[{i}] Score: {result['score']:.3f}")
                print(f"    –ò—Å—Ç–æ—á–Ω–∏–∫: {result['source']} (—á–∞–Ω–∫ {result['chunk_index']})")
                print(f"    {result['content'][:200]}...\n")

        elif command == "test":
            # –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
            test_search()

        else:
            print(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞: {command}")
            print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã: index, search, test")

    else:
        print("RAG –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ MivDating")
        print("\n–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
        print("  python rag_indexer.py index           # –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã")
        print("  python rag_indexer.py search '–∑–∞–ø—Ä–æ—Å' # –ü–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É")
        print("  python rag_indexer.py test            # –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫")


if __name__ == "__main__":
    main()
