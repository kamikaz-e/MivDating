# Интеграция удаленного сервера - Завершено

## Что было сделано

### 1. Создан Flask API сервер

**Файл:** `ollama_api_server.py`

Сервер проксирует все запросы к локальному Ollama:
- `/health` - проверка состояния
- `/api/tags` - список моделей
- `/api/chat` - чат запросы
- `/api/embeddings` - эмбеддинги
- `/api/generate` - генерация текста

**Особенности:**
- Поддержка streaming и non-streaming режимов
- Таймауты: 5 минут для чата, 1 минута для эмбеддингов
- Логирование всех запросов
- Обработка ошибок

**Файлы сервера:**
- `ollama_api_server.py` - основной код сервера
- `requirements.txt` - зависимости Python
- `run_server.sh` - скрипт запуска через gunicorn
- `REMOTE_SERVER_SETUP.md` - инструкция по настройке

### 2. Модифицировано Android приложение

**Изменения в OllamaClient.kt:**
- Добавлен метод `updateBaseUrl()` для динамической смены URL
- URL теперь можно менять без пересоздания клиента
- Автоматический сброс кеша модели при смене URL

**Изменения в OllamaViewModel.kt:**
- Автоматическая загрузка URL из настроек при инициализации
- Метод `updateOllamaUrl()` для сохранения нового URL
- Перепроверка подключения при смене URL

**Изменения в OllamaUrlHelper.kt:**
- Добавлена константа `REMOTE_SERVER_URL` с адресом удаленного сервера
- Обновлены инструкции для пользователей

**Новый UI:**
- `SettingsDialog.kt` - диалог настройки подключения
- Три режима: Локальный эмулятор, Удаленный сервер, Пользовательский URL
- Кнопка настроек в ChatScreen

**Обновлен ChatScreen.kt:**
- Добавлена иконка настроек в TopAppBar
- Интеграция SettingsDialog

### 3. Документация

**REMOTE_SERVER_SETUP.md:**
- Пошаговая инструкция установки сервера
- Настройка systemd для автозапуска
- Конфигурация файрвола
- Примеры тестирования
- Troubleshooting

**ANDROID_APP_SETUP.md:**
- Инструкция по настройке приложения
- Режимы работы (локальный/удаленный)
- Управление разрешениями
- Тестирование подключения
- Troubleshooting

## Архитектура

```
┌─────────────────────┐
│  Android App        │
│  (Kotlin + Ktor)    │
└──────────┬──────────┘
           │ HTTP POST/GET
           │
           ▼
┌─────────────────────────────┐
│  Flask API Server           │
│  130.49.153.154:8000        │
│  - /api/chat                │
│  - /api/embeddings          │
│  - /api/tags                │
│  - /health                  │
└──────────┬──────────────────┘
           │ HTTP Proxy
           │
           ▼
┌─────────────────────────────┐
│  Ollama                     │
│  localhost:11434            │
│  - qwen2.5:14b              │
│  - nomic-embed-text         │
└─────────────────────────────┘
```

## Следующие шаги

### Настройка удаленного сервера

1. Подключитесь к серверу:
   ```bash
   ssh user@130.49.153.154
   ```

2. Следуйте инструкциям в `REMOTE_SERVER_SETUP.md`:
   - Установите Ollama
   - Загрузите модели
   - Скопируйте файлы сервера
   - Настройте systemd
   - Откройте порт 8000

3. Запустите сервисы:
   ```bash
   sudo systemctl start ollama
   sudo systemctl start ollama-api
   ```

4. Проверьте работу:
   ```bash
   curl http://130.49.153.154:8000/health
   ```

### Настройка приложения

1. Запустите приложение на эмуляторе или устройстве

2. Откройте настройки (иконка шестеренки в TopAppBar)

3. Выберите "Удаленный сервер" или введите пользовательский URL

4. Нажмите "Сохранить"

5. Проверьте статус подключения в чате

### Тестирование

1. Отправьте тестовое сообщение в чат

2. Проверьте логи на сервере:
   ```bash
   sudo journalctl -u ollama-api -f
   ```

3. Проверьте логи в Android Studio (Logcat):
   ```
   adb logcat -s OllamaClient RAGViewModel
   ```

## Возможные проблемы и решения

### Сервер не отвечает

**Проблема:** Приложение не может подключиться к серверу

**Решение:**
1. Проверьте, что сервер запущен: `systemctl status ollama-api`
2. Проверьте файрвол: `sudo ufw status`
3. Проверьте URL в настройках приложения
4. Убедитесь, что устройство имеет доступ к интернету

### Модель не найдена

**Проблема:** Ошибка "Model not found"

**Решение:**
1. На сервере: `ollama list`
2. Если модели нет: `ollama pull qwen2.5:14b`

### Таймаут

**Проблема:** Запрос прерывается по таймауту

**Решение:**
1. Проверьте ресурсы сервера (RAM, CPU)
2. Увеличьте таймаут в `OllamaClient.kt`
3. Используйте менее ресурсоемкую модель

### Cleartext traffic

**Проблема:** Android блокирует HTTP запросы

**Решение:**
Добавьте в `AndroidManifest.xml`:
```xml
<application android:usesCleartextTraffic="true" ...>
```

## Production рекомендации

Для production использования рекомендуется:

1. **HTTPS:** Настройте nginx с SSL сертификатом
2. **Аутентификация:** Добавьте API ключи или OAuth
3. **Rate Limiting:** Ограничьте количество запросов
4. **Мониторинг:** Настройте Prometheus + Grafana
5. **Кеширование:** Кешируйте эмбеддинги и часто используемые ответы
6. **Load Balancing:** Используйте несколько инстансов для высокой нагрузки
7. **Backup:** Регулярно делайте бэкап индексов и данных

## Дополнительные возможности

### Режим работы оффлайн

Приложение может работать в 3 режимах:
1. **Полностью онлайн:** Удаленный сервер + RAG на сервере
2. **Гибридный:** Удаленный сервер + локальный RAG
3. **Полностью оффлайн:** Локальный Ollama + локальный RAG

### Синхронизация данных

Можно добавить синхронизацию:
- История чата между устройствами
- Индексы RAG
- Настройки пользователя

### Аналитика

Добавьте сбор метрик:
- Время ответа модели
- Количество запросов
- Популярные запросы
- Ошибки

## Заключение

Интеграция завершена. Приложение теперь может работать как с локальным Ollama, так и с удаленным сервером. Пользователь может легко переключаться между режимами через UI.

Для запуска в production следуйте рекомендациям безопасности и производительности.
